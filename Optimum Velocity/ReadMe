Methodology of Q-Learning


Q-learning is one of the most used algorithms among other Reinforcement Learning (RL) techniques due to its model free nature.
This technique mimics the human learning procedure and is able to accomplish an arbitrary task from experience gained by direct
interaction with the environment. The â€œlearnerâ€ is called as the agent. Everything that the agent interacts with, is called a state,
where states represent situations of the environment and everything the agent do, is called an action, where actions represent the 
choices of the agent.
The knowledge obtained through the procedure of training, for every pair of state (s) and action (a), is called a Q-value (s,a)
and for the purpose of this system it is saved in a table data structure, called as Q-Table. At the beginning of training this 
table has been initialized with zeros. The control problems goal is to find all the pairs (s, a) which give
the best overall reward to the agent. The sequence of those tuples (s,a) is called as the optimal policy of the problem.
The optimal policy for every state is calculated as:

              ğœ‹ (ğ‘ ) = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ (ğ‘ ,ğ‘) 

The Q-value associated with every (s,a) pair is updated every time the agent make a choice:

        ğ‘„t(ğ‘ , ğ‘) = ğ‘„t-1(ğ‘ , ğ‘) + l[ğ‘Ÿ + ğ›¾ğ‘šğ‘ğ‘¥ğ‘„t-1(ğ‘ â€²,ğ‘â€²) âˆ’ ğ‘„t-1(ğ‘ , ğ‘)]

Here Î³ is a discount factor of value 0â‰¤Î³â‰¤1, l is the learning rate of value 0â‰¤Î±â‰¤1, ğ‘Ÿ is the reward of the action and ğ‘šğ‘ğ‘¥ğ‘„t-1(ğ‘ â€²,ğ‘â€²)
is the maximum Q value of the state ğ‘ â€² which correspond to the state that the specific action Î± will lead to.
The primary challenge of the agent arise when has to make a choice of action. The simplest selection strategy is called greedy 
and assures that the agent at any given time choose the action with the highest Q-value in order to maximize the short-term reward.
However the agent has to experiment with different combinations of action in order to discover the optimal policy.
The solution to this problem is to balance the exploitation and the exploration by behaving most of the time greedily 
while choosing a uniform random action with small a probability É›. This strategy is called as Îµ-greedy:

        ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘¡ğ‘’ğ‘”ğ‘¦ {ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘š ğ‘â„ğ‘œğ‘–ğ‘ ğ‘’ ğ‘œğ‘“ ğ‘ ğ‘¤ğ‘–ğ‘¡â„ ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ ğ‘ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘„(ğ‘ ,ğ‘) ğ‘¤ğ‘–ğ‘¡â„ ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ 1âˆ’ğ‘

The probability that I used is:

       ğ‘=ğ‘’âˆ’ğ‘›Ã—ğœ€ 
       
Here n is the number of iterations executed from the algorithm and Îµ the exploration rate.


Pseudo Code of Q-Learning Algorithm
Input: set of states (s), set of actions (a), reward function (r)
Input: discount factor (Î³), learning rate (l), exploration rate (Îµ)
Initialise: Q(s,Î±), polisy (Ï€)
Repeat:
For each state:
Choose an action (a) from the current state (s) based on Îµ
Calculate the reward for the new action
Update Q-Table
Ï€ (s)=argmax Q(s,a)
Until Ï€(s) has converged
